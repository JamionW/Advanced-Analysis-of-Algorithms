{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1v3ixPaZz7Xew0q_MGVydG2MtXbzHihuQ",
      "authorship_tag": "ABX9TyOJ4H5VctoL0OHUMmI1y4sR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamionW/Advanced-Analysis-of-Algorithms/blob/master/Master_Code_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This is the master notebook."
      ],
      "metadata": {
        "id": "J8JM9NHrENit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osmnx # install the osmnx module\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OPd3P4QfmuCg",
        "outputId": "91b95d22-b30e-428e-a0dd-7338d523824c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting osmnx\n",
            "  Downloading osmnx-1.9.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: geopandas<0.15,>=0.12 in /usr/local/lib/python3.10/dist-packages (from osmnx) (0.14.4)\n",
            "Requirement already satisfied: networkx<3.4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from osmnx) (3.3)\n",
            "Requirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from osmnx) (1.26.4)\n",
            "Requirement already satisfied: pandas<2.3,>=1.1 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.1.4)\n",
            "Requirement already satisfied: requests<2.33,>=2.27 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.32.3)\n",
            "Requirement already satisfied: shapely<2.1,>=2.0 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.0.5)\n",
            "Requirement already satisfied: fiona>=1.8.21 in /usr/local/lib/python3.10/dist-packages (from geopandas<0.15,>=0.12->osmnx) (1.9.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas<0.15,>=0.12->osmnx) (24.1)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas<0.15,>=0.12->osmnx) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1->osmnx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1->osmnx) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1->osmnx) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<2.33,>=2.27->osmnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<2.33,>=2.27->osmnx) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<2.33,>=2.27->osmnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<2.33,>=2.27->osmnx) (2024.7.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas<0.15,>=0.12->osmnx) (24.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas<0.15,>=0.12->osmnx) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas<0.15,>=0.12->osmnx) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas<0.15,>=0.12->osmnx) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas<0.15,>=0.12->osmnx) (1.16.0)\n",
            "Downloading osmnx-1.9.4-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: osmnx\n",
            "Successfully installed osmnx-1.9.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import fiona\n",
        "import numpy as np\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "from shapely.ops import nearest_points, linemerge, transform\n",
        "from shapely.geometry import Point, LineString\n",
        "from geopandas.tools import sjoin_nearest\n",
        "from scipy.spatial import cKDTree\n",
        "from collections import defaultdict\n",
        "from pyproj import CRS, Transformer\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "JWtJRCd_XtzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_nearest_linestring_efficient(gdf_points, gdf_lines, max_distance):\n",
        "    \"\"\"\n",
        "    Find the nearest linestring for each point, up to a maximum distance.\n",
        "    Uses spatial indexing for efficiency.\n",
        "\n",
        "    :param gdf_points: GeoDataFrame with point geometries (in UTM)\n",
        "    :param gdf_lines: GeoDataFrame with linestring geometries (in UTM)\n",
        "    :param max_distance: Maximum distance to consider (in meters)\n",
        "    :return: GeoDataFrame with points matched to nearest linestrings\n",
        "    \"\"\"\n",
        "    # Use sjoin_nearest to find the nearest linestring for each point\n",
        "    joined = sjoin_nearest(gdf_points, gdf_lines, max_distance=max_distance, how='left')\n",
        "\n",
        "    # Calculate the actual distances\n",
        "    joined['distance'] = joined.apply(lambda row: row['geometry'].distance(gdf_lines.loc[row['index_right'], 'geometry'])\n",
        "                                      if pd.notnull(row['index_right']) else None, axis=1)\n",
        "\n",
        "    # Remove matches beyond max_distance (should be unnecessary due to max_distance in sjoin_nearest, but just in case)\n",
        "    joined = joined[joined['distance'] <= max_distance]\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    result = joined.drop(columns=['index_right', 'distance'])\n",
        "\n",
        "    print(f\"Matched {result.notna().any(axis=1).sum()} out of {len(result)} points\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "LTt0bcyHfgGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset imports"
      ],
      "metadata": {
        "id": "yIq_DbPjYEUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADDRESSES\n",
        "\n",
        "# Read in addresses\n",
        "# this takes about 20 minutes for the State of Tennessee\n",
        "# less than a minute for Chattanooga\n",
        "\n",
        "# Read the GeoJSON file into a GeoDataFrame\n",
        "#address_df = gpd.read_file('/content/drive/MyDrive/Colab Notebooks/data/tennessee.geojson')\n",
        "\n",
        "# Chattanooga, for testing\n",
        "address_df = gpd.read_file('/content/drive/MyDrive/Colab Notebooks/data/chattanooga.geojson')\n"
      ],
      "metadata": {
        "id": "SuqI7At-EL4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVI\n",
        "\n",
        "# Path to .gdb file\n",
        "gdb_file = \"/content/drive/MyDrive/Colab Notebooks/data/SVI2022_TENNESSEE_tract.gdb\"\n",
        "\n",
        "# List all the layers in the .gdb file\n",
        "layers = fiona.listlayers(gdb_file)\n",
        "print(\"Layers in the geodatabase:\", layers)\n",
        "\n",
        "# Read the desired layer\n",
        "svi_df = gpd.read_file(gdb_file, layer='SVI2022_TENNESSEE_tract')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBAGbLaBLuLQ",
        "outputId": "d146c9c6-66a4-451a-87c6-553909fe3600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers in the geodatabase: ['SVI2022_TENNESSEE_tract']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROADS\n",
        "\n",
        "# Import shapefiles\n",
        "# https://www.census.gov/cgi-bin/geo/shapefiles/index.php\n",
        "\n",
        "# documentation here: https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2023/TGRSHP2023_TechDoc.pdf\n",
        "\n",
        "# Open the shapefile as a Fiona collection\n",
        "with fiona.open('/content/drive/MyDrive/Colab Notebooks/data/tl_2023_47065_roads.shp') as collection:\n",
        "    # Create a GeoDataFrame from the collection\n",
        "    roads_df = gpd.GeoDataFrame.from_features(collection)\n"
      ],
      "metadata": {
        "id": "4m23qzVl8xeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AMENITIES\n",
        "\n",
        "city = \"Chattanooga, Tennessee, USA\"\n",
        "tags = {'amenity': ['school', 'hospital', 'library'],\n",
        "        'shop': 'supermarket'}\n",
        "\n",
        "amenities = ox.features_from_place(city, tags=tags)"
      ],
      "metadata": {
        "id": "dx9LyIdJnCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Coordinate Reference Systems"
      ],
      "metadata": {
        "id": "Se46c8etpXbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the original CRS\n",
        "print(\"Original CRS:\", roads_df.crs)\n",
        "\n",
        "# If the CRS is None, set it to WGS84 (assuming that's what it should be)\n",
        "if roads_df.crs is None:\n",
        "    roads_df.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the target CRS (UTM zone 18N)\n",
        "target_crs = CRS(\"EPSG:32618\")\n",
        "\n",
        "# Perform the transformation\n",
        "roads_df_transformed = roads_df.to_crs(target_crs)\n",
        "\n",
        "# Check the new CRS\n",
        "print(\"New CRS:\", roads_df_transformed.crs)\n",
        "\n",
        "# Print a sample of the transformed geometries\n",
        "print(\"Sample of transformed geometries:\")\n",
        "print(roads_df_transformed['geometry'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILa_VMzhskhY",
        "outputId": "8cf7c43b-3150-4e34-a5fd-6d1c05c68a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original CRS: None\n",
            "New CRS: EPSG:32618\n",
            "Sample of transformed geometries:\n",
            "0    LINESTRING (-438047.238 3925110.410, -438015.6...\n",
            "1    LINESTRING (-438047.238 3925110.410, -437997.8...\n",
            "2    LINESTRING (-438379.567 3925013.438, -438362.5...\n",
            "3    LINESTRING (-434083.556 3936175.511, -434066.9...\n",
            "4    LINESTRING (-436868.331 3941879.564, -436868.2...\n",
            "Name: geometry, dtype: geometry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bounding box of the data:\")\n",
        "print(roads_df.total_bounds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3pf9hqsvgE",
        "outputId": "32829292-ae24-4317-cf96-1fa7a650a72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bounding box of the data:\n",
            "[-85.469528  34.982924 -84.94233   35.459232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the coordinate reference systems\n",
        "latlong_crs = CRS(\"EPSG:4326\")  # WGS84 lat/long\n",
        "utm_crs = CRS(\"EPSG:32618\")  # UTM zone 18N\n",
        "\n",
        "address_df = address_df.to_crs(utm_crs)\n",
        "svi_df = svi_df.to_crs(utm_crs)\n",
        "amenities = amenities.to_crs(utm_crs)\n",
        "\n",
        "print(address_df.crs)\n",
        "print(svi_df.crs)\n",
        "print(roads_df_transformed.crs)\n",
        "print(amenities.crs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvjcHAcpDHC",
        "outputId": "721f5dee-8412-4904-ccd1-30d7e7f2ea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPSG:32618\n",
            "EPSG:32618\n",
            "EPSG:32618\n",
            "EPSG:32618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Engineering: SVI Filtering"
      ],
      "metadata": {
        "id": "qg-8a-HPZKa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all columns from the svi_df dataframe except \"geometry\",\"STATE\",\"ST_ABBR\",\"COUNTY\",\"FIPS\",\"LOCATION\",\"AREA_SQMI\", and \"RPL_THEME4\".\n",
        "\n",
        "svi_df = svi_df[[\"geometry\",\"STATE\",\"ST_ABBR\",\"COUNTY\",\"FIPS\",\"LOCATION\",\"AREA_SQMI\", \"RPL_THEME4\"]]\n"
      ],
      "metadata": {
        "id": "Cx56hr8iiiZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Engineering: Amenities cleanup"
      ],
      "metadata": {
        "id": "1vPIBzVBa93z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where the 'amenity' column is null\n",
        "joined_amenities_df = amenities.dropna(subset=['amenity'])\n",
        "amenities = amenities.dropna(subset=['amenity'])\n",
        "print(f\"Number of amenities after removing null 'amenity' values: {len(amenities)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiP1vcOJQwjv",
        "outputId": "49e83f64-c758-443a-c95b-12699bc79190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of amenities after removing null 'amenity' values: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of valid values in the amenity column of joined_amenities_df\n",
        "\n",
        "print(amenities['amenity'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bRlpMi7UQtA",
        "outputId": "2a2bfec7-acf2-453f-cec6-cdbc38e7d65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amenity\n",
            "school        85\n",
            "hospital      10\n",
            "library        5\n",
            "restaurant     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter amenities\n",
        "amenity_types = ['school', 'library', 'hospital']\n",
        "filtered_amenities_df = amenities[amenities['amenity'].isin(amenity_types)]\n",
        "\n",
        "print(f\"Number of filtered amenities: {len(filtered_amenities_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6D4lN9zQ4XL",
        "outputId": "cb370a54-273f-4c8a-e7db-3ccc841adffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of filtered amenities: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering: Address Density"
      ],
      "metadata": {
        "id": "QEMFlyyHZWjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### This is to engineer the feature for address density\n",
        "\n",
        "# Extract coordinates from the geometry column\n",
        "coords = np.array(list(address_df.geometry.apply(lambda x: (x.x, x.y))))\n",
        "\n",
        "# Build the KD-tree\n",
        "tree = cKDTree(coords)\n",
        "\n",
        "# Set the buffer distance (e.g., 1000 meters)\n",
        "buffer_distance = 1000\n",
        "\n",
        "# Query the tree for all points within the buffer distance\n",
        "indices = tree.query_ball_point(coords, r=buffer_distance)\n",
        "\n",
        "# Count the number of neighbors, excluding the point itself\n",
        "address_df['address_density'] = [len(idx) - 1 for idx in indices]\n",
        "\n",
        "# Optionally, normalize the density\n",
        "max_density = address_df['address_density'].max()\n",
        "address_df['normalized_density'] = address_df['address_density'] / max_density\n",
        "\n",
        "# Print some statistics\n",
        "print(address_df['address_density'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKaeCUjTu0qa",
        "outputId": "ae1092b0-c0e5-40d6-d03b-2c14d45a91e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    102761.000000\n",
            "mean       1235.861251\n",
            "std         643.406327\n",
            "min           0.000000\n",
            "25%         729.000000\n",
            "50%        1196.000000\n",
            "75%        1663.000000\n",
            "max        3676.000000\n",
            "Name: address_density, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Join on geometry attributes\n",
        "\n",
        "joined_svi_address_df = gpd.overlay(address_df, svi_df, how='intersection')"
      ],
      "metadata": {
        "id": "SykvOYFjL_nb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Place Addresses and Amenities on a graph"
      ],
      "metadata": {
        "id": "vkAHew74ag4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a graph from a GeoDataFrame of roads\n",
        "def create_graph_from_roads(roads_gdf):\n",
        "    G = nx.Graph()\n",
        "    for idx, row in roads_gdf.iterrows():\n",
        "        if row.geometry.geom_type == 'LineString':\n",
        "            start = row.geometry.coords[0]\n",
        "            end = row.geometry.coords[-1]\n",
        "            G.add_edge(start, end, geometry=row.geometry, length=row.geometry.length)\n",
        "        elif row.geometry.geom_type == 'MultiLineString':\n",
        "            merged = linemerge(row.geometry)\n",
        "            if merged.geom_type == 'LineString':\n",
        "                start = merged.coords[0]\n",
        "                end = merged.coords[-1]\n",
        "                G.add_edge(start, end, geometry=merged, length=merged.length)\n",
        "            else:\n",
        "                for line in merged.geoms:\n",
        "                    start = line.coords[0]\n",
        "                    end = line.coords[-1]\n",
        "                    G.add_edge(start, end, geometry=line, length=line.length)\n",
        "    return G\n",
        "\n",
        "# Create the graph\n",
        "G = create_graph_from_roads(roads_df_transformed)"
      ],
      "metadata": {
        "id": "JuINL-cu6MEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import cKDTree\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import multiprocessing\n",
        "\n",
        "def add_points_to_graph_optimized(G, gdf_points, n_jobs=None):\n",
        "    # Determine the number of jobs\n",
        "    if n_jobs is None or n_jobs <= 0:\n",
        "        n_jobs = multiprocessing.cpu_count()  # Use all available CPUs\n",
        "\n",
        "    # Extract node coordinates and create a KD-tree\n",
        "    nodes = np.array([(n[0], n[1]) for n in G.nodes if isinstance(n, tuple) and len(n) == 2])\n",
        "    tree = cKDTree(nodes)\n",
        "\n",
        "    # Extract point coordinates\n",
        "    points = np.array([(p.x, p.y) for p in gdf_points.geometry])\n",
        "\n",
        "    def process_point(idx, point):\n",
        "        try:\n",
        "            # Find nearest node\n",
        "            distance, nearest_index = tree.query(point, k=1)\n",
        "            nearest_node = tuple(nodes[nearest_index])\n",
        "\n",
        "            # Create point node\n",
        "            point_node = f\"point_{idx}\"\n",
        "            point_data = gdf_points.iloc[idx].to_dict()\n",
        "\n",
        "            return point_node, nearest_node, distance, point, point_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing point {idx}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # Use parallel processing to handle points\n",
        "    with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
        "        futures = [executor.submit(process_point, idx, point) for idx, point in enumerate(points)]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                point_node, nearest_node, distance, point, point_data = result\n",
        "                G.add_node(point_node, geometry=point, point_data=point_data)\n",
        "                G.add_edge(point_node, nearest_node, length=distance)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Usage\n",
        "G = add_points_to_graph_optimized(G, joined_svi_address_df)"
      ],
      "metadata": {
        "id": "-52NuCuQyQAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to do this in order to work with the Point geographies within the amenities polygons\n",
        "\n",
        "filtered_amenities_df = filtered_amenities_df.copy()\n",
        "filtered_amenities_df.loc[:, 'geometry'] = filtered_amenities_df.geometry.centroid"
      ],
      "metadata": {
        "id": "RNqes8v04OYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the amenities to the Graph\n",
        "\n",
        "G = add_points_to_graph_optimized(G, filtered_amenities_df)"
      ],
      "metadata": {
        "id": "Xe2MsHXn1vNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further Feature Engineering"
      ],
      "metadata": {
        "id": "uK_L6VXKDoep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def engineer_features(df, G):\n",
        "    # Start with relevant numerical features\n",
        "    features = df[['address_density', 'normalized_density', 'AREA_SQMI']].copy()\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    categorical_features = ['STATE', 'ST_ABBR', 'COUNTY']\n",
        "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "    encoded_features = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "    # Check which method is available and use it\n",
        "    if hasattr(encoder, 'get_feature_names_out'):\n",
        "        feature_names = encoder.get_feature_names_out(categorical_features)\n",
        "    else:\n",
        "        feature_names = encoder.get_feature_names(categorical_features)\n",
        "\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n",
        "\n",
        "    # Combine numerical and encoded categorical features\n",
        "    features = pd.concat([features, encoded_df], axis=1)\n",
        "\n",
        "    # Extract amenity distances from the graph\n",
        "    amenity_types = ['grocery', 'hospital', 'school', 'park']  # Add or remove types as needed\n",
        "    max_distance = 1000000  # A large number to represent \"unreachable\"\n",
        "\n",
        "    for amenity_type in amenity_types:\n",
        "        features[f'distance_to_nearest_{amenity_type}'] = df.apply(\n",
        "            lambda row: get_nearest_amenity_distance(G, (row.geometry.x, row.geometry.y), amenity_type, max_distance),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Calculate local network characteristics\n",
        "    for node in G.nodes():\n",
        "        if 'geometry' in G.nodes[node]:\n",
        "            features.at[node, 'local_edge_density'] = len(list(G.edges(node))) / G.graph['area']\n",
        "            features.at[node, 'avg_neighbor_degree'] = nx.average_neighbor_degree(G, nodes=[node])[node]\n",
        "\n",
        "    return features\n",
        "\n",
        "def get_nearest_amenity_distance(G, source, amenity_type, max_distance):\n",
        "    try:\n",
        "        distances = [nx.shortest_path_length(G, source, target, weight='length')\n",
        "                     for target in G.nodes()\n",
        "                     if G.nodes[target].get('type') == amenity_type]\n",
        "        return min(distances) if distances else max_distance\n",
        "    except nx.NetworkXNoPath:\n",
        "        return max_distance\n",
        "\n",
        "# Apply the feature engineering\n",
        "feature_df = engineer_features(joined_svi_address_df, G)\n",
        "\n",
        "print(feature_df.head())\n",
        "\n",
        "# Store RPL_THEME4 separately as our reference\n",
        "reference_svi = joined_svi_address_df['RPL_THEME4']"
      ],
      "metadata": {
        "id": "MzzAeOZCDwTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6dc152-d7f8-438a-f8c1-8aed2b75eada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Edge Accessibility (in development, not working, need parallelism)"
      ],
      "metadata": {
        "id": "oXTunBc-zTSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "# def calculate_ea(G, origins, destinations, omega=1.28, max_workers=None):\n",
        "#     \"\"\"\n",
        "#     Calculate Edge Accessibility for the graph G.\n",
        "\n",
        "#     :param G: NetworkX graph\n",
        "#     :param origins: List of origin nodes\n",
        "#     :param destinations: List of destination nodes\n",
        "#     :param omega: Impedance parameter (default: 1.28)\n",
        "#     :param max_workers: Number of parallel processes to use\n",
        "#     :return: Dictionary of EA values for each edge\n",
        "#     \"\"\"\n",
        "#     def process_edge(edge):\n",
        "#         u, v = edge\n",
        "#         original_length = G[u][v]['length']\n",
        "\n",
        "#         # Remove the edge\n",
        "#         G.remove_edge(u, v)\n",
        "\n",
        "#         ea_value = 0\n",
        "#         for origin in origins:\n",
        "#             for dest in destinations:\n",
        "#                 # Skip if origin and destination are the same\n",
        "#                 if origin == dest:\n",
        "#                     continue\n",
        "\n",
        "#                 try:\n",
        "#                     # Calculate shortest path length with the edge removed\n",
        "#                     length_without_edge = nx.shortest_path_length(G, origin, dest, weight='length')\n",
        "\n",
        "#                     # Get origin and destination attributes\n",
        "#                     origin_data = G.nodes[origin].get('point_data', {})\n",
        "#                     dest_data = G.nodes[dest].get('point_data', {})\n",
        "\n",
        "#                     # Extract multipliers and weights\n",
        "#                     mu_i = origin_data.get('address_density', 1)  # Use address density as multiplier\n",
        "#                     rho_i = origin_data.get('normalized_density', 1)  # Use normalized density as weight\n",
        "#                     gamma_j = 1  # Assuming no destination multiplier\n",
        "#                     phi_j = dest_data.get('amenity_importance', 1)  # You'll need to add this attribute to amenities\n",
        "\n",
        "#                     # Calculate EA contribution\n",
        "#                     ea_value += (mu_i * gamma_j * rho_i * phi_j) * (1 / (length_without_edge ** omega))\n",
        "#                 except nx.NetworkXNoPath:\n",
        "#                     # If no path exists, contribution is 0\n",
        "#                     pass\n",
        "\n",
        "#         # Restore the edge\n",
        "#         G.add_edge(u, v, length=original_length)\n",
        "\n",
        "#         return edge, ea_value\n",
        "\n",
        "#     edges = list(G.edges())\n",
        "#     ea_values = {}\n",
        "\n",
        "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "#         future_to_edge = {executor.submit(process_edge, edge): edge for edge in edges}\n",
        "#         for future in as_completed(future_to_edge):\n",
        "#             edge, value = future.result()\n",
        "#             ea_values[edge] = value\n",
        "\n",
        "#     return ea_values"
      ],
      "metadata": {
        "id": "5_MXqkdCzwux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing only: filter and export"
      ],
      "metadata": {
        "id": "gED-caNGbtVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This calls the function from the beginning which joins a shapefile dataset to a linestring set\n",
        "joined_svi_df = find_nearest_linestring_efficient(joined_svi_address_df, roads_df_transformed, 200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOi0E8bysKiH",
        "outputId": "3369195e-bc4a-4add-a8d7-5082584a1e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched 0 out of 0 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0ee733655df3>:24: FutureWarning: <class 'geopandas.array.GeometryArray'>._reduce will require a `keepdims` parameter in the future\n",
            "  print(f\"Matched {result.notna().any(axis=1).sum()} out of {len(result)} points\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This calls the function from the beginning which joins a shapefile dataset to a linestring set\n",
        "joined_amenities_df = find_nearest_linestring_efficient(amenities, roads_df_transformed, 200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMsc5cC7Q0FS",
        "outputId": "3b1f4aa9-7efd-4fde-871c-0314aee3bc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched 0 out of 0 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0ee733655df3>:24: FutureWarning: <class 'geopandas.array.GeometryArray'>._reduce will require a `keepdims` parameter in the future\n",
            "  print(f\"Matched {result.notna().any(axis=1).sum()} out of {len(result)} points\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to my address (for testing)\n",
        "\n",
        "selected_records = final_results[(final_results['street'] == 'TUCKER ST')] # & (final_results['number'] == '335')]\n"
      ],
      "metadata": {
        "id": "N0Kz06o8AhJK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the selected_record dataframe above to a csv file\n",
        "\n",
        "selected_records.to_csv('selected_records.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ED5cFo7bBREq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115d11ee-c53c-44a2-80fa-a20deb1bc73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wp1VqcinCDz7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}